
#Importation des bibliothèques Pandas 
#Pour gérer l'importation Spacy 
#Pour faire le nettoyage en_core_web_sm 
#Pour gérer la bibliothèque de mot en anglais 
#Wordcloud pour créer un nuage de mot 
#Matplot pour créer des graphiques 
#CountVectorizer a la même fonction que Spacy, je la garde au cas où avec une fonction particulière pour vectoriser a nouveau 
#Time pour gérer le temps 
#TextBlob pour voir la polarité (methode .sentiment)

En cas de besoin
#pip install spacy
#python -m spacy download en_core_web_sm

Reste à faire sur le pre-process : 
- Vérification de la distribution des commentaires (si cela suit la loi de bernoulli on peut prendre le bernoulli naive bayse) mais on peut également prendre que N élément du dataset en toute confiance
- Voir l'utilité des N-gram (sinon on passe au TF-IDF pour réduire la fréquence des mots comme use)
- Voir si on prend les ADC également au risque d'aloudir et de bruiter le corpus nlp
- utilité de passer au NLTK pour voir la différence avec SPACY (voir combiner les deux)
- Essayer éventuellement le stem au lieu du lemma

Priorité : 
- Voir le résultat d'un NB sur le dataset initial puis le clean ?

Tâche suivante : 
- Exécuter tt les classifier
- Voir le besoin ou non de boosting
- executer un vote classifier (méthode utilisant le pipeline ?)

import pandas as pd
import re
import spacy
import en_core_web_sm
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import CountVectorizer
from time import time
import numpy as np
from textblob import TextBlob

#Importation du fichier csv
aws = pd.read_csv("pr_10k.csv",sep=";",header=0)

#On peut faire une pré-exploration des 9999 lignes.
#On sait par exemple qu'il y a 23 lignes vides dans la colonne review qu'on peut nettoyer. 

print(aws.head(10))
print(aws.dtypes)
print(aws.info())
print(aws.describe())

print(aws.groupby(['sentiment']).count())

#On voit qu'il y a une disparité entre les sentiments négatifs et les positifs. Il faudra y remédier avant l'intégration dans le training set.

#Il faut soit :
#- Générer de la données 
#- Limiter le nombre de commentaires positifs

#En attendant :
#- On supprime les lignes vides 
#- On se débarasse également de la colonne name contenant les produits (et qui ne va pas servir)

aws = aws.drop(['name'],axis=1)
aws =aws.dropna()

#On passe la colonne Review en minuscule et on stock les nouveaux commentaires dans une nouvelle colonne : Commentaire nettoyé

aws['clean review'] = aws['review'].str.lower()

#On génère la vectorisation des commentaires et la tokenisation

nlp = en_core_web_sm.load()
t=time()
aws['clean review'] = aws['clean review'].apply(nlp)
t1=time()
print("NLP : " + str(t1-t))

#Generer une liste de Token + lemmatize(>1 sec)
#Generer une liste de Token + Lemmatize + POS (>1 sec ??)
#Generer une liste de Token + Lemmatize + POS + stopword (>1 sec)

keep=['VERB','ADJ','ADV']
stopwords = spacy.lang.en.stop_words.STOP_WORDS
t=time()
aws['Lemma'] = aws['clean review'].apply(lambda x:[token.lemma_ for token in x])
aws['Pos'] = aws['clean review'].apply(lambda x:[token.pos_ for token in x])
aws['clean review'] = aws['clean review'].apply(lambda x:[token.lemma_ for token in x if token.lemma_.isalpha() and token.pos_ in keep and token not in stopwords])
t1=time()
print("Token :" +str(t1-t))

#Vérification d'après nettoyage

aws['len'] = aws['clean review'].apply(lambda x : len(x))
aws['empty']= aws['clean review'].apply(lambda x : bool(len(x) == 0))

print(aws.iloc[1288])
print(' ')
print(aws.groupby(['empty']).count())
print(' ')
print(aws.loc[aws['empty'] == True])

#Générer une longue chaine de caractère
s = " "
long_string = s.join(aws['review'])

#Générer un nuage de mot
wordcloud = WordCloud()
wordcloud.generate(long_string)

#Visualiser le nuage
wordcloud.to_image()

#Transformer clean review en chaine de caractère
def listToString(s):  
    
    # initialize an empty string 
    str1 = ""  
    
    # traverse in the string   
    for ele in s:  
        str1 += ele + " "   
    
    # return string   
    return str1 

aws['clean review'] = aws['clean review'].apply(lambda x: listToString(x))

#Générer un nuage de mot après nettoyage
#Générer une longue chaine de caractère
s = " "
clean_string = s.join(aws['clean review'])

#Générer un nuage de mot
wordcloud = WordCloud()
wordcloud.generate(clean_string)

#Visualiser le nuage
wordcloud.to_image()

#Fonction pour la visualisation des mots les plus communs du corpus

#Visualisation des mots les plus communs
def plot_most_common_words(count_data, count_vectorizer):
    words = count_vectorizer.get_feature_names()
    total_counts = np.zeros(len(words))
    for t in count_data:
        total_counts+=t.toarray()[0]
    
    count_dict = (zip(words, total_counts))
    count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[0:30]
    words = [w[0] for w in count_dict]
    counts = [w[1] for w in count_dict]
    x_pos = np.arange(len(words)) 

    plt.bar(x_pos, counts,align='center')
    plt.xticks(x_pos, words, rotation=90) 
    plt.xlabel('Mots')
    plt.ylabel('Fréquence')
    plt.title('Les 30 mots les plus communs')
    plt.show()
    
 #Visualisation des mots les plus communs avant nettoyage
 # Initialise the count vectorizer with the English stop words
count_vectorizer = CountVectorizer(stop_words='english')

# Fit and transform 
count_data = count_vectorizer.fit_transform(aws['review'])

# Visualise
plot_most_common_words(count_data, count_vectorizer)

#Visualisation après nettoyage
# Initialise the count vectorizer with the English stop words
count_vectorizer = CountVectorizer(stop_words='english')

# Fit and transform 
count_data = count_vectorizer.fit_transform(aws['clean review'])

# Visualise
plot_most_common_words(count_data, count_vectorizer)

#Matrice mot/documents après nettoyage
vect = CountVectorizer()
vect.fit(aws['clean review'])
test= vect.transform(aws['clean review'])
my_array = test.toarray()
test_df = pd.DataFrame(my_array, columns=vect.get_feature_names())

#nombre de mot
test_df.info()

#Voir le corpus
print(vect.get_feature_names())

#fonction pour définir les mots communs
def dict_common_words(count_data, count_vectorizer):
    words = count_vectorizer.get_feature_names()
    total_counts = np.zeros(len(words))
    for t in count_data:
        total_counts+=t.toarray()[0]
    
    count_dict = (zip(words, total_counts))
    count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[0:300]
    words = [w[0] for w in count_dict]
    counts = [w[1] for w in count_dict]
    
    return (count_dict)

print(dict_common_words(count_data, count_vectorizer))

#Test de polarité avec TextBlob

aws['polarity_Subjectivity'] = aws['clean review'].apply(lambda x: TextBlob(x).sentiment)
print(aws.head())
















